{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa02ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (1.8.1)\r\n",
      "Requirement already satisfied: toml in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (0.10.2)\r\n",
      "Requirement already satisfied: emoji in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (2.11.0)\r\n",
      "Requirement already satisfied: requests in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (2.28.1)\r\n",
      "Requirement already satisfied: numpy in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (1.23.5)\r\n",
      "Requirement already satisfied: networkx in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (2.8.4)\r\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (1.12.1)\r\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (5.26.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from stanza) (4.64.1)\r\n",
      "Requirement already satisfied: typing_extensions in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from requests->stanza) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from requests->stanza) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from requests->stanza) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages (from requests->stanza) (2022.12.7)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01261794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javierchengkaiheng/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 57.0MB/s]\n",
      "2024-04-09 21:29:33 INFO: Downloaded file to /Users/javierchengkaiheng/stanza_resources/resources.json\n",
      "2024-04-09 21:29:33 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-09 21:29:35 INFO: File exists: /Users/javierchengkaiheng/stanza_resources/en/default.zip\n",
      "2024-04-09 21:29:40 INFO: Finished downloading models and saved to /Users/javierchengkaiheng/stanza_resources\n",
      "2024-04-09 21:29:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 49.6MB/s]\n",
      "2024-04-09 21:29:40 INFO: Downloaded file to /Users/javierchengkaiheng/stanza_resources/resources.json\n",
      "2024-04-09 21:29:42 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-04-09 21:29:42 INFO: Using device: cpu\n",
      "2024-04-09 21:29:42 INFO: Loading: tokenize\n",
      "2024-04-09 21:29:42 INFO: Loading: mwt\n",
      "2024-04-09 21:29:42 INFO: Loading: pos\n",
      "2024-04-09 21:29:43 INFO: Loading: lemma\n",
      "2024-04-09 21:29:43 INFO: Loading: constituency\n",
      "2024-04-09 21:29:43 INFO: Loading: depparse\n",
      "2024-04-09 21:29:43 INFO: Loading: sentiment\n",
      "2024-04-09 21:29:44 INFO: Loading: ner\n",
      "2024-04-09 21:29:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Barack', 4, 'nsubj:pass')\n",
      "('Obama', 1, 'flat')\n",
      "('was', 4, 'aux:pass')\n",
      "('born', 0, 'root')\n",
      "('in', 6, 'case')\n",
      "('Hawaii', 4, 'obl')\n",
      "('.', 4, 'punct')\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc98d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThe, DET, det\n",
      "\tmagnetic, ADJ, amod\n",
      "flux, NOUN, compound\n",
      "density, NOUN, nsubj:pass\n",
      "\tis, AUX, aux:pass\n",
      "\tdefined, VERB, root\n",
      "\tas, ADP, case\n",
      "\tthe, DET, det\n",
      "force, NOUN, obl\n",
      "\tper, ADP, case\n",
      "unit, NOUN, compound\n",
      "length, NOUN, nmod\n",
      "\tper, ADP, case\n",
      "unit, NOUN, compound\n",
      "current, NOUN, nmod\n",
      "\tacting, VERB, advcl\n",
      "\ton, ADP, case\n",
      "\ta, DET, det\n",
      "\tstraight, ADJ, amod\n",
      "\tcurrent, ADJ, amod\n",
      "\t-, PUNCT, punct\n",
      "\tcarrying, VERB, amod\n",
      "conductor, NOUN, obl\n",
      "\tplaced, VERB, acl\n",
      "\tperpendicular, ADJ, xcomp\n",
      "\tto, ADP, case\n",
      "\ta, DET, det\n",
      "\tmagnetic, ADJ, amod\n",
      "field, NOUN, obl\n",
      "\t., PUNCT, punct\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "#assuming NLP pipeline for english has already been initialised\n",
    "\n",
    "def extract_words(input_text):\n",
    "\n",
    "    # Process the input text\n",
    "    doc = nlp(input_text)\n",
    "\n",
    "    # Extract nouns (NN, NNS, NNP, NNPS)\n",
    "    ##ADJ for adjectives\n",
    "    ##N for nouns\n",
    "    nouns = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.upos.startswith('N') or word.upos == \"PROPN\":\n",
    "                print(f\"{word.text}, {word.upos}, {word.deprel}\")\n",
    "            else:\n",
    "                print(f\"\\t{word.text}, {word.upos}, {word.deprel}\")\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"The magnetic flux density is defined as the force per unit length per unit current acting on a straight current-carrying conductor placed perpendicular to a magnetic field.\"\n",
    "nouns_list = extract_words(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3988f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj(input_text):\n",
    "    doc = nlp(input_text)\n",
    "    long = []\n",
    "    for sentence in doc.sentences:\n",
    "        out = \"\"\n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"ADJ\":\n",
    "                out += f\"{word.text} \"\n",
    "            elif word.upos.startswith(\"N\"):\n",
    "                out += f\"{word.text} \"\n",
    "            if word.upos == \"AUX\":\n",
    "                long.append(out)\n",
    "                break\n",
    "    if long:\n",
    "        return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791fa195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['naturalistic fallacy ']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"\"\"\n",
    "The naturalistic fallacy: Just because this is the way things CURRENTLY ARE does not mean this is the way things OUGHT TO BE.\n",
    "\"\"\"\n",
    "print(extract_subj(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3f4d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Assuming stanza is imported\n",
    "def extract_subj_v2(input_text):\n",
    "    #Assuming stanza pipeline for english has already been initialised\n",
    "    long_linetext = re.sub(r'[\\n\\r]+',': ',input_text)\n",
    "    # print(long_linetext)\n",
    "    doc = nlp(long_linetext)\n",
    "    long = []\n",
    "    for sentence in doc.sentences:\n",
    "        out = \"\"\n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"ADJ\":\n",
    "                out += f\"{word.text} \"\n",
    "            elif word.upos == \"ADP\":\n",
    "                out += f\"{word.text} \"\n",
    "            elif word.upos.startswith(\"N\") or word.upos == \"PROPN\":\n",
    "                out += f\"{word.text} \"\n",
    "            elif word.upos == \"AUX\" or word.text==\":\":\n",
    "                if out != '': \n",
    "                    long.append({'':out,word.upos:word.text})\n",
    "                break\n",
    "\n",
    "        # if out != '': long.append({\"subj\":out,word.upos:word.text})\n",
    "    if long:\n",
    "        return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c2ea963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the [{'': 'equation for Force due to acceleration ', 'AUX': 'is'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_sentence = \"\"\"The equation for Force only due to acceleration is $F=ma$\"\"\"\n",
    "print(f\"What is the {extract_subj_v2(input_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda3b31",
   "metadata": {},
   "source": [
    "# Flashcard Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829149b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 20.9MB/s]\n",
      "2024-04-09 21:32:37 INFO: Downloaded file to /Users/javierchengkaiheng/stanza_resources/resources.json\n",
      "2024-04-09 21:32:37 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-09 21:32:39 INFO: File exists: /Users/javierchengkaiheng/stanza_resources/en/default.zip\n",
      "2024-04-09 21:32:44 INFO: Finished downloading models and saved to /Users/javierchengkaiheng/stanza_resources\n",
      "2024-04-09 21:32:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 47.0MB/s]\n",
      "2024-04-09 21:32:44 INFO: Downloaded file to /Users/javierchengkaiheng/stanza_resources/resources.json\n",
      "2024-04-09 21:32:46 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-04-09 21:32:46 INFO: Using device: cpu\n",
      "2024-04-09 21:32:46 INFO: Loading: tokenize\n",
      "2024-04-09 21:32:46 INFO: Loading: mwt\n",
      "2024-04-09 21:32:46 INFO: Loading: pos\n",
      "2024-04-09 21:32:47 INFO: Loading: lemma\n",
      "2024-04-09 21:32:47 INFO: Loading: constituency\n",
      "2024-04-09 21:32:47 INFO: Loading: depparse\n",
      "2024-04-09 21:32:48 INFO: Loading: sentiment\n",
      "2024-04-09 21:32:48 INFO: Loading: ner\n",
      "2024-04-09 21:32:49 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd501768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "#Assuming stanza is imported\n",
    "def make_flashcard_v1(input_text):\n",
    "    #Assuming stanza pipeline for english has already been initialised\n",
    "    long_linetext = re.sub(r'[\\n\\r]+',': ',input_text)\n",
    "    # print(long_linetext)\n",
    "    doc = nlp(long_linetext)\n",
    "    for sentence in doc.sentences:\n",
    "        front = \"What is \"\n",
    "        back = \"It is \"\n",
    "        separator_id = 0\n",
    "        flashcard=[]\n",
    "        for word in sentence.words:\n",
    "            if word.upos == \"ADJ\":\n",
    "                front += f\"{word.text} \"\n",
    "            elif word.upos == \"ADP\":\n",
    "                front += f\"{word.text} \"\n",
    "            elif word.upos.startswith(\"N\") or word.upos == \"PROPN\":\n",
    "                front += f\"{word.text} \"\n",
    "            elif word.upos == \"DET\":\n",
    "                front += f\"{word.text.lower()} \"\n",
    "            elif word.upos == \"AUX\" or word.text==\":\":\n",
    "                if front != '': \n",
    "                    front = front.strip()+\"?\"\n",
    "                    flashcard.append(front)\n",
    "                    back += long_linetext[long_linetext.find(f'{word.text} ')+len(word.text)+1:]\n",
    "                    flashcard.append(back)\n",
    "                return flashcard\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1f6145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the equation for Force due to acceleration?', 'It is $F=ma$ which is also the rate of change of momentum over time.']\n",
      "['What is History?', 'It is a verbal structure in the form of a narrative prose discourse that purports to be a model or icon of past structures.']\n",
      "['What is the magnetic flux linkage?', 'It is defined as the product of the number of turns of the coil and the magnetic flux through each turn.']\n",
      "['What is a historian?', 'It is only access them indirectly, and any account of these events is thus an invention rather than a representation of the past. ']\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_sentence1 = \"The equation for Force only due to acceleration is $F=ma$ which is also the rate of change of momentum over time.\"\n",
    "input_sentence2 = \"History is a verbal structure in the form of a narrative prose discourse that purports to be a model or icon of past structures.\"\n",
    "input_sentence3 = \"The magnetic flux linkage is defined as the product of the number of turns of the coil and the magnetic flux through each turn.\"\n",
    "\n",
    "#IT WORKS FOR 'WHAT' SENTENCES, it can work for definitions and descriptions, anything that can be phrased as a 'what' question.\n",
    "\n",
    "input_sentence4 = \"A historian cannot access past events directly, they can only access them indirectly, and any account of these events is thus an invention rather than a representation of the past. \"\n",
    "input_sentence5 = \"Alternating current occurs when charge carriers periodically reverse their direction of motion.\"\n",
    "input_sentence6 = \"The side that the induced current points to has higher potential, because the current pushes electrons away from that side.\"\n",
    "\n",
    "print(make_flashcard_v1(input_sentence1))\n",
    "print(make_flashcard_v1(input_sentence2))\n",
    "print(make_flashcard_v1(input_sentence3))\n",
    "print(make_flashcard_v1(input_sentence4))\n",
    "print(make_flashcard_v1(input_sentence5))\n",
    "print(make_flashcard_v1(input_sentence6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc18a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
